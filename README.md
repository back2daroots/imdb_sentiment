# IMDb Sentiment Analysis â€” NLP Baseline Pipeline

A clean, reproducible ML pipeline for Sentiment Analysis on the IMDb movie reviews dataset.  
This project demonstrates a script-first workflow with configuration files, reproducible CV folds, OOF predictions, and baseline evaluation.

## ğŸ“Œ Project Overview

This project explores sentiment analysis on the IMDb movie reviews dataset.
The goal is to compare classical NLP approaches with transformer-based models,
and to investigate how ensembling and conditional blending can improve performance.

We start with a TF-IDF + Logistic Regression baseline, move to a DistilBERT model,
and then combine both using global and conditional blending strategies.

---
## ğŸ“¦ Project Structure
```
imdb/
â”œâ”€â”€ configs/                 # YAML configs
â”‚   â””â”€â”€ base.yaml
â”œâ”€â”€ data/                    # Train / test splits
â”œâ”€â”€ models/                  # Saved models and OOF predictions
â”œâ”€â”€ reports/                 # Metrics, confusion matrices, error analysis
â”œâ”€â”€ scripts/                 # Training, evaluation and analysis scripts
â”‚   â”œâ”€â”€ 02_train_tfidf_lr.py
â”‚   â”œâ”€â”€ 03_eval.py
â”‚   â”œâ”€â”€ 05_train_distilbert.py
â”‚   â”œâ”€â”€ 06_eval_distilbert.py
â”‚   â”œâ”€â”€ 07_blend_models.py
â”‚   â”œâ”€â”€ 08_error_analysis.py
â”‚   â””â”€â”€ 09_conditional_blend.py
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py
â”‚   â””â”€â”€ experiment_logger.py
â””â”€â”€ README.md
```
---

## ğŸ“¥ Dataset

The project accepts one of the following formats inside `data/`:

- `imdb.csv` â€” with columns `review`/`text` and `sentiment`/`label`  
  *(this is the format currently used)*  
- or `train.csv` + `test.csv`
- or classic IMDb folder structure:
train/pos&neg
test/pos&neg
---
## ğŸ§ª Pipeline Overview
- Automatically detects dataset format (imdb.csv, train/test CSV, or IMDb folders) and builds:
	â€¢	data/train.csv
	â€¢	data/test.csv
	â€¢	data/cv_folds.json (reproducible stratified K-Fold splits)
- Trains 5 fold models, computes OOF predictions, and stores models + metrics.
- Loads models, ensembles them (mean of probabilities), computes metrics, and saves a confusion matrix.

---

## ğŸ¤— Transformer Baseline â€” DistilBERT

In addition to the TF-IDF + Logistic Regression baseline, we trained a transformer-based model using:

- **distilbert-base-uncased**
- **same 5-fold CV splits** (`cv_folds.json`)
- fine-tuning performed on **Google Colab (A100 GPU)**

Training script:
python -m scripts.05_train_distilbert

Evaluation:
python -m scripts.06_eval_distilbert

---

## ğŸ”¥ Comparison: TF-IDF Baseline vs DistilBERT

| Model                       | F1     | ROC-AUC | Accuracy |
|-----------------------------|--------|---------|----------|
| **TF-IDF + Logistic Reg.**  | 0.9161 | 0.9742  | 0.9141   |
| **DistilBERT (transformer)**| **0.9216** | **0.9765** | **0.9207** |

DistilBERT outperforms the TF-IDF baseline across all major metrics:
- +0.5% F1
- +0.6% Accuracy
- +0.23% ROC-AUC

This confirms the correctness of the training pipeline and demonstrates the expected benefit of a transformer-based approach.

---
## ğŸ§© Blending: TF-IDF + DistilBERT

We blended TF-IDF+LogReg and DistilBERT probabilities:

p_blend = (1 - alpha) * p_tfidf + alpha * p_distilbert

The weight \(\alpha\) was selected by maximizing **OOF F1** (using the same CV folds).

Run:
```bash
python -m scripts.07_blend_models
```

ğŸ“ˆ Blend â€” Test Metrics

- F1:       0.9369
- ROC-AUC:  0.9815
- Accuracy: 0.9357

This blended model substantially outperforms both individual models.

---
## ğŸ† Model Comparison (Test Set)

| Model                         | F1     | ROC-AUC | Accuracy |
|------------------------------|--------|---------|----------|
| TF-IDF + Logistic Regression | 0.9141 | 0.9742  | 0.9141   |
| DistilBERT                   | 0.9216 | 0.9765  | 0.9207   |
| Global Blend                 | 0.9369 | 0.9815  | 0.9357   |
| Conditional Blend (length)   | 0.9390 | 0.9819  | 0.9380   |

---

ğŸ•µï¸ Error Analysis Summary

We compared predictions across TF-IDF, DistilBERT, and the blend:
- DistilBERT improves especially on long, descriptive, context-dependent reviews, where sentiment is expressed implicitly and requires understanding the overall tone and argument structure.
- TF-IDF remains strong on short and emotionally explicit reviews, where single keywords (e.g., â€œawfulâ€, â€œexcellentâ€) carry most of the signal.
- Both models struggle with sarcasm/irony, mixed-sentiment reviews, and potential label noise.

As a result, blending benefits from complementary strengths and achieves the best overall quality.

---

## ğŸ§  Conditional Blending (by review length)

We applied a simple rule-based blend depending on review length (word count):

- If `len_words(text) <= N`: use `alpha_short`
- Else: use `alpha_long`

Parameters were selected by maximizing **OOF F1**.

Best OOF parameters:
- N = 200 words
- alpha_short = 0.50
- alpha_long = 0.20

ğŸ“ˆ Conditional Blend â€” Test Metrics

- F1:       0.9390
- ROC-AUC:  0.9819
- Accuracy: 0.9380

Run:
```bash
python -m scripts.09_conditional_blend
```

---

## ğŸ¯ Next Steps

- **Further transformer tuning** (epochs, lr, max_length, scheduler)

---

## ğŸ”‘ Key Takeaways

- Classical TF-IDF models remain strong on short and emotionally explicit texts.
- Transformer models better capture contextual and implicit sentiment.
- Blending complementary models yields significant gains without retraining.
- Simple rule-based conditional blending can further improve performance.

------


## Setup
```bash
python -m venv .venv
source .venv/bin/activate   # Win: .\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
